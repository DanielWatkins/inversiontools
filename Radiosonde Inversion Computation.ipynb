{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is for processing output files from the University of Wyoming \n",
    "# radiosonde record. It assumes that all of the records are in a single\n",
    "# folder. Each file has up to a year's worth of observations and each\n",
    "# file must strictly adhere to the UW archive format, as this code assumes\n",
    "# things like the number of spaces between columns and the number of metadata\n",
    "# rows at the end.\n",
    "#\n",
    "# getIDLines(path to single file) --> list of the indices of the first lines of \n",
    "#                                     individual soundings\n",
    "#\n",
    "#\n",
    "# fetchSingleRecord(path to a single file, beginning row, ending row) \n",
    "#                                 --> dataframe with one data from one sounding, \n",
    "#                                     and a dictionary with metadata\n",
    "#\n",
    "# computeInversionQuantities(dataframe with sounding, metadata) \n",
    "#                                 -->  dictionary with base_height, strength, \n",
    "#                                      depth, and length (length being the number\n",
    "#                                      of observations)\n",
    "#\n",
    "# main(name of file to create, path to data file) --> CSV with inversion values\n",
    "#\n",
    "# Additional filters could go into main()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 'Radiosonde Data/' + f_list entry = fname\n",
    "def getIDLines(fname):\n",
    "    \"\"\"Each text file has up to a year's worth of radiosonde\n",
    "    profiles. The record analyzer only can handle a single\n",
    "    sounding at a time. So we need to find the start and stop lines\n",
    "    for each sounding. fname is the path to the file to analyze.\"\"\"\n",
    "    \n",
    "    fid = open(fname)\n",
    "    lines = fid.readlines() # returns a list containing all lines of text from fname\n",
    "    station_id = lines[0].split()[0]\n",
    "    \n",
    "    id_loc = []\n",
    "    for x in range(0,len(lines)):\n",
    "        # The reason for the try/except thing is that if consists only of white space\n",
    "        # or has a single entry then the return from the .split() will not be subsettable.\n",
    "        # In that case, it doesn't matter, since it also won't be a header line\n",
    "        try:\n",
    "            if lines[x].split()[0] == station_id:\n",
    "                id_loc.append(x)\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "    return id_loc + [len(lines)]\n",
    "\n",
    "\n",
    "\n",
    "# Function to process a single sounding.\n",
    "\n",
    "def fetchSingleRecord(fname, header_row, end_row):\n",
    "    \"\"\"Reads and processes a single sounding from the U Wyoming \n",
    "    weather station data. It is important that the sounding be in\n",
    "    the exact UW format. Since output data can have a full year worth\n",
    "    of soundings in a single file, specifying the line number where the header\n",
    "    is located (header_row) and the line number of the next sounding's header\n",
    "    (end_row) is necessary. Output is a dataframe with the information from the\n",
    "    sounding table, but not including the analysis or metadata. \n",
    "    \n",
    "    Much of this code is directly copied from the SkewT python package.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(fname) as fid:\n",
    "        lines = fid.readlines() # returns a list containing all lines of text from fname\n",
    "    \n",
    "    # Spacing is consistent, these are the left and right sides of the columns\n",
    "    lhi=[1, 9,16,23,30,37,46,53,58,65,72]\n",
    "    rhi=[7,14,21,28,35,42,49,56,63,70,77]\n",
    "\n",
    "    # initialise output data structure\n",
    "    output={}\n",
    "\n",
    "    header=lines[header_row].split()\n",
    "    station_id = header[0]\n",
    "    date = pd.to_datetime(' '.join(header[-3:] +  [header[-4]]))\n",
    "    fields=lines[header_row + 2].split()\n",
    "    units=lines[header_row + 3].split()\n",
    "\n",
    "    for ff in fields:\n",
    "        output[ff]=[] # Initialize container for fields\n",
    "    \n",
    "        \n",
    "        # For each of the lines with observations, split the lines and save\n",
    "        # the data as float in the output dictionary. The last 25 lines are always\n",
    "        # the calculations and metadata.\n",
    "        \n",
    "    for line,idx in zip(lines[header_row + 5:],range(header_row, end_row-25)):\n",
    "  \n",
    "        # If pressure field is missing, skip\n",
    "        try: \n",
    "            output[fields[0]].append(float(line[lhi[0]:rhi[0]]))\n",
    "        except ValueError: \n",
    "            break # I think this would just go to the next line \n",
    "            # This may be the time to ID the line \"Station information..\"\n",
    "\n",
    "        for ii in range(1,len(rhi)):\n",
    "            try: \n",
    "                textdata=line[lhi[ii]:rhi[ii]].strip()\n",
    "                output[fields[ii]].append(float(textdata))\n",
    "            except ValueError: \n",
    "                output[fields[ii]].append(np.NaN)\n",
    "        \n",
    "\n",
    "    # Create Pandas dataframe \n",
    "    df = pd.DataFrame(columns=fields, index=np.arange(0,len(output[fields[0]])))\n",
    "\n",
    "    for ff in fields:\n",
    "        df[ff] = output[ff]\n",
    "    \n",
    "    # Now grab the metadata:\n",
    "    station_metadata = {}\n",
    "    station_metadata['date'] = date\n",
    "    still_looking = True\n",
    "    i = header_row\n",
    "    while still_looking: \n",
    "        # Find the second header so we can count backwards\n",
    "        if lines[i].split()[0] == 'Station':\n",
    "            if lines[i].split()[1] == 'number:':\n",
    "                # could have backup loop to ensure no errors here, too\n",
    "                station_metadata['number'] = lines[i].split()[2]\n",
    "                station_metadata['latitude'] = float(lines[i+2].split()[2])\n",
    "                station_metadata['longitude'] = float(lines[i+3].split()[2])\n",
    "                station_metadata['elevation'] = float(lines[i+4].split()[2])\n",
    "                still_looking = False\n",
    "        i += 1\n",
    "    \n",
    "    return (df, station_metadata)\n",
    "\n",
    "def computeInversionQuantities(df,meta):\n",
    "    \"\"\"Calculate, if present, the inversion base height,\n",
    "    inversion depth, and inversion strength for a sounding \n",
    "    stored in df. Df and meta are the output from the fetchSingleRecord\n",
    "    function. Records number of measurements in the sounding.\n",
    "    Returns a dictionary with keys base_height, depth, strength, and length.\"\"\"\n",
    "    \n",
    "    # Todo: require inversions to be less than a certain height\n",
    "    # Inversion base must be below 700 hPa\n",
    "    # Todo: allow for small non-inversion layers\n",
    "    # Threshold is 100 m\n",
    "    # Vector with temperatures must have at least 10 observations.\n",
    "    \n",
    "    # Only include observations above or equal to the station elevation.\n",
    "    valid = df.HGHT >= meta['elevation']\n",
    "    tvec1 = np.array(df.TEMP[valid])\n",
    "    zvec1 = np.array(df.HGHT[valid])\n",
    "    pvec1 = np.array(df.PRES[valid])\n",
    "    \n",
    "    # Drop missing obvservations\n",
    "    valid = ~(np.isnan(tvec1) | np.isnan(zvec1))\n",
    "    tvec = tvec1[valid]\n",
    "    zvec = zvec1[valid]\n",
    "    pvec = pvec1[valid]\n",
    "    \n",
    "    # If there are insufficient observations to compute gradient, return NaNs.\n",
    "    length = len(tvec)\n",
    "    if length <= 2:\n",
    "        return {'base_height':np.NaN, 'depth':np.NaN, 'strength':np.NaN,  \n",
    "                'length':len(tvec), 'base_press':np.NaN}\n",
    "   \n",
    "\n",
    "    # Next we look for an inversion\n",
    "    diffsign = np.sign(np.diff(tvec))\n",
    "    i = 0\n",
    "    inversion_present = True\n",
    "    \n",
    "    while diffsign[i] < 0:\n",
    "    # This will find, if present, the base height of the inversion layer\n",
    "    # If no inversion is present, then it will be flagged\n",
    "        i += 1\n",
    "        if i == len(diffsign):\n",
    "            inversion_present = False\n",
    "            break\n",
    "\n",
    "    # If there is indeed an inversion, find the base height, depth, and strength\n",
    "    if inversion_present:\n",
    "        base_height = zvec[i] - zvec[0] # There are a couple cases where this is negative... how?\n",
    "        if base_height < 0:\n",
    "            print(meta['number'])\n",
    "        base_press = pvec[i]\n",
    "        \n",
    "        # Now look for the top of the inversion\n",
    "        # The inversion top is where diffsign < 0.\n",
    "        # If diffsign < 0, check if the next layer is an inversion layer\n",
    "        for j in range(i,len(diffsign)):\n",
    "            if diffsign[j] < 0:\n",
    "                if (j+2 < len(diffsign)):\n",
    "                    if (diffsign[j+1] > 0) & (zvec[j+2]-zvec[j+1] < 100):\n",
    "                        continue\n",
    "                    else:\n",
    "                        depth = zvec[j] - zvec[i]\n",
    "                        strength = tvec[j] - tvec[i]\n",
    "                        break\n",
    "                else:\n",
    "                    depth = zvec[j] - zvec[i]\n",
    "                    strength = tvec[j] - tvec[i]\n",
    "                    break\n",
    "                        \n",
    "        if j == len(diffsign)-1: \n",
    "            if diffsign[j] >= 0: # Then inversion top is beyond the last observation\n",
    "                depth = np.NaN\n",
    "                strength = np.NaN    \n",
    "                \n",
    "    # If no inversion is present, then the variables are undefined\n",
    "    else:\n",
    "        base_height, depth, strength, base_press = np.NaN, np.NaN, np.NaN, np.NaN\n",
    "    \n",
    "    return {'base_height':base_height, 'depth':depth, 'strength':strength, 'length':length,'base_press':base_press}\n",
    "   \n",
    "\n",
    "def main(datapath, savepath):\n",
    "    # Datapath: path to folder containing radiosonde soundings\n",
    "    # savepath: path + filename for new file\n",
    "\n",
    "    f_list = os.listdir(datapath)\n",
    "    f_list = f_list[1:] # The first item in the list is always '.DS_Store', which we don't need.\n",
    "\n",
    "    # Initialize the pandas dataframe\n",
    "    columns = ['date', 'elevation', 'latitude', \n",
    "               'longitude','number','base_height', \n",
    "               'depth','strength','length']\n",
    "\n",
    "    big_df = pd.DataFrame(np.NaN, columns=columns,index=[0])\n",
    "    big_df = big_df[columns] # ensure that the column order is correct\n",
    "    big_df.to_csv(savepath, header=True, index=False)\n",
    "    \n",
    "    idx = 1\n",
    "\n",
    "    for fname in f_list:\n",
    "        id_loc = getIDLines(datapath + '/' + fname)\n",
    "        for i in range(0,len(id_loc)-1):\n",
    "            df, meta = fetchSingleRecord(datapath + '/' + fname, header_row=id_loc[i], end_row=id_loc[i+1])\n",
    "            inv_df = computeInversionQuantities(df,meta)\n",
    "            small_df = pd.DataFrame(dict(inv_df, **meta), index=[idx])\n",
    "            small_df = small_df[columns] # ensure columns in right order\n",
    "            big_df = pd.concat([big_df, small_df])\n",
    "            idx += 1\n",
    "\n",
    "        # Write result after each year\n",
    "        with open('radsonde_inv_analysis.csv','a') as f:\n",
    "            big_df.drop([0]).to_csv(f,header=False,index=False)\n",
    "\n",
    "        # Reset big_df\n",
    "        big_df = pd.DataFrame(np.NaN, columns=columns,index=[0])\n",
    "\n",
    "datapath = '/Users/watkdani/Google Drive/Research/Radiosonde Analysis/Test Data'\n",
    "savepath = '/Users/watkdani/Google Drive/Research/Radiosonde Analysis/test_data.csv'\n",
    "main(datapath, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating lookup table for the radiosonde data, this can later be joined with the radsonde analysis csv\n",
    "def read_station_list(fname):\n",
    "    \"\"\"Read the weather station code table and put into a dataframe.\"\"\"\n",
    "    import numpy.ma as ma\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    with open(fname) as fid:\n",
    "        lines = fid.readlines() # returns a list containing all lines of text from fname\n",
    "    \n",
    "    # Spacing is consistent, these are the left and right sides of the columns\n",
    "    columns = ['call','number','name','st/pr','country','latitude','longitude','elevation']\n",
    "    lhi=[0, 10, 16, 49, 52, 55, 62, 69]\n",
    "    rhi=[4, 15, 48, 51, 54, 60, 67, 73]\n",
    "\n",
    "    # initialise output data structure\n",
    "    df = pd.DataFrame(columns=columns, index=np.arange(0,len(lines)))\n",
    "     \n",
    "    for line,idx in zip(lines,range(0, len(lines))):\n",
    "  \n",
    "        \n",
    "        try: \n",
    "            output[columns[0]].append(float(line[lhi[0]:rhi[0]]))\n",
    "        except ValueError: \n",
    "            break # I think this would just go to the next line \n",
    "            # This may be the time to ID the line \"Station information..\"\n",
    "\n",
    "        for ii in range(1,len(rhi)):\n",
    "            try: \n",
    "                textdata=line[lhi[ii]:rhi[ii]].strip()\n",
    "                output[fields[ii]].append(float(textdata))\n",
    "            except ValueError: \n",
    "                output[fields[ii]].append(np.NaN)\n",
    "        \n",
    "\n",
    "    for ff in fields:\n",
    "        df[ff] = output[ff]\n",
    "    \n",
    "    # Now grab the metadata:\n",
    "    station_metadata = {}\n",
    "    station_metadata['date'] = date\n",
    "    still_looking = True\n",
    "    i = header_row\n",
    "    while still_looking: \n",
    "        # Find the second header so we can count backwards\n",
    "        if lines[i].split()[0] == 'Station':\n",
    "            if lines[i].split()[1] == 'number:':\n",
    "                # could have backup loop to ensure no errors here, too\n",
    "                station_metadata['number'] = lines[i].split()[2]\n",
    "                station_metadata['latitude'] = float(lines[i+2].split()[2])\n",
    "                station_metadata['longitude'] = float(lines[i+3].split()[2])\n",
    "                station_metadata['elevation'] = float(lines[i+4].split()[2])\n",
    "                still_looking = False\n",
    "        i += 1\n",
    "    \n",
    "    return (df, station_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "fname = '/Users/watkdani/Google Drive/Research/Radiosonde Analysis/snstns.tbl'\n",
    "with open(fname) as fid:\n",
    "    lines = fid.readlines() # returns a list containing all lines of text from fname\n",
    "\n",
    "# Spacing is consistent, these are the left and right sides of the columns\n",
    "columns = ['call','number','name','st/pr','country','latitude','longitude','elevation']\n",
    "lhi=[0, 10, 16, 49, 52, 55, 62, 69]\n",
    "rhi=[4, 15, 48, 51, 54, 60, 67, 73]\n",
    "\n",
    "# initialise output data structure\n",
    "df = pd.DataFrame(columns=columns, index=np.arange(0,len(lines)))\n",
    "\n",
    "\n",
    "for line,idx in zip(lines,range(0, len(lines))):\n",
    "    \n",
    "    for ii in range(len(columns)):\n",
    "        df.loc[idx,columns[ii]] = line[lhi[ii]:rhi[ii]]\n",
    "    \n",
    "#     if len(line.split()) == 9: # nothing missing\n",
    "#         df.loc[idx, columns] = line.split()[0:8]\n",
    "#     elif len(line.split()) == 8: # no call sign. \n",
    "#         df.loc[idx, columns[1:]] = line.split()[0:7]\n",
    "#     else:\n",
    "#         pass\n",
    "    \n",
    "df.loc[df['st/pr'] == '--', 'st/pr'] = np.NaN\n",
    "for idx in range(len(lines)):\n",
    "#     latstring = df.loc[idx,'latitude']\n",
    "#     latfloat = float(latstring[0:-2].strip() + '.' + latstring[-2:].strip())\n",
    "#     df.loc[idx,'latitude'] = latfloat\n",
    "#     lonstring = df.loc[idx,'longitude']\n",
    "#     lonfloat = float(lonstring[0:-2].strip() + '.' + lonstring[-2:].strip())\n",
    "#     df.loc[idx,'longitude'] = lonfloat\n",
    "    df.loc[idx,'latitude'] = convert_degree_string_to_float(df.loc[idx,'latitude'])\n",
    "    df.loc[idx,'longitude'] = convert_degree_string_to_float(df.loc[idx,'longitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_degree_string_to_float(dmstr):\n",
    "    \"\"\" Lat and lon are strings in the format (s)DDdd and (s)DDDdd where (s) is \n",
    "    either ' ' or '-', and need to be converted to DD.dd / DDD.dd respectively.\n",
    "    \"\"\"\n",
    "    DD = dmstr[0:-2]\n",
    "    dd = dmstr[-2:]\n",
    "    if len(DD.strip()) > 0:\n",
    "        numeric_degrees = float(DD.strip() + '.' + dd)\n",
    "    elif dd.strip()[0] == '-':\n",
    "        numeric_degrees = -float('.' + dd.strip()[1:])\n",
    "    else:\n",
    "        numeric_degrees = float('.' + dd.strip())\n",
    "    return numeric_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>number</th>\n",
       "      <th>name</th>\n",
       "      <th>st/pr</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>PABR</td>\n",
       "      <td>70026</td>\n",
       "      <td>BARROW/W._POST_W.ROGERS</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>71.28</td>\n",
       "      <td>156.79</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td></td>\n",
       "      <td>70027</td>\n",
       "      <td>NORTH SLOPE</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>71.32</td>\n",
       "      <td>156.62</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>PAOT</td>\n",
       "      <td>70133</td>\n",
       "      <td>KOTZEBUE,_RALPH_WIEN</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>66.86</td>\n",
       "      <td>162.63</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td></td>\n",
       "      <td>70197</td>\n",
       "      <td>CENTRAL</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>65.48</td>\n",
       "      <td>144.66</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>PAOM</td>\n",
       "      <td>70200</td>\n",
       "      <td>NOME</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>64.5</td>\n",
       "      <td>165.43</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>PABE</td>\n",
       "      <td>70219</td>\n",
       "      <td>BETHEL/BETHEL_AIRPORT</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>60.78</td>\n",
       "      <td>161.84</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>PAMC</td>\n",
       "      <td>70231</td>\n",
       "      <td>MCGRATH</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>62.96</td>\n",
       "      <td>155.61</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td></td>\n",
       "      <td>70252</td>\n",
       "      <td>TALKEETNA</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>62.3</td>\n",
       "      <td>150.41</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>PAFA</td>\n",
       "      <td>70261</td>\n",
       "      <td>FAIRBANKS/INT</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>64.81</td>\n",
       "      <td>147.88</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>PABI</td>\n",
       "      <td>70266</td>\n",
       "      <td>FORT_GREELY</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>63.96</td>\n",
       "      <td>145.7</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td></td>\n",
       "      <td>70268</td>\n",
       "      <td>GLENNALLEN</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>62.1</td>\n",
       "      <td>145.96</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>PANC</td>\n",
       "      <td>70273</td>\n",
       "      <td>ANCHORAGE/INT</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>61.16</td>\n",
       "      <td>150.01</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>PASN</td>\n",
       "      <td>70308</td>\n",
       "      <td>ST._PAUL</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>57.16</td>\n",
       "      <td>170.22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>PACD</td>\n",
       "      <td>70316</td>\n",
       "      <td>COLD_BAY</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>55.2</td>\n",
       "      <td>162.71</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>PAKN</td>\n",
       "      <td>70326</td>\n",
       "      <td>KING_SALMON</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>58.68</td>\n",
       "      <td>156.67</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>PADQ</td>\n",
       "      <td>70350</td>\n",
       "      <td>KODIAK</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>57.75</td>\n",
       "      <td>152.5</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>PAYA</td>\n",
       "      <td>70361</td>\n",
       "      <td>YAKUTAT</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>59.51</td>\n",
       "      <td>139.67</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>PANT</td>\n",
       "      <td>70398</td>\n",
       "      <td>ANNETTE_ISLAND</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>55.05</td>\n",
       "      <td>131.59</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>PASY</td>\n",
       "      <td>70414</td>\n",
       "      <td>SHEMYA_AFB</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>52.71</td>\n",
       "      <td>174.1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      call number                              name st/pr country latitude  \\\n",
       "1011  PABR  70026  BARROW/W._POST_W.ROGERS             AK      US    71.28   \n",
       "1012        70027  NORTH SLOPE                         AK      US    71.32   \n",
       "1013  PAOT  70133  KOTZEBUE,_RALPH_WIEN                AK      US    66.86   \n",
       "1014        70197  CENTRAL                             AK      US    65.48   \n",
       "1015  PAOM  70200  NOME                                AK      US     64.5   \n",
       "1016  PABE  70219  BETHEL/BETHEL_AIRPORT               AK      US    60.78   \n",
       "1017  PAMC  70231  MCGRATH                             AK      US    62.96   \n",
       "1018        70252  TALKEETNA                           AK      US     62.3   \n",
       "1019  PAFA  70261  FAIRBANKS/INT                       AK      US    64.81   \n",
       "1020  PABI  70266  FORT_GREELY                         AK      US    63.96   \n",
       "1021        70268  GLENNALLEN                          AK      US     62.1   \n",
       "1022  PANC  70273  ANCHORAGE/INT                       AK      US    61.16   \n",
       "1023  PASN  70308  ST._PAUL                            AK      US    57.16   \n",
       "1024  PACD  70316  COLD_BAY                            AK      US     55.2   \n",
       "1025  PAKN  70326  KING_SALMON                         AK      US    58.68   \n",
       "1026  PADQ  70350  KODIAK                              AK      US    57.75   \n",
       "1027  PAYA  70361  YAKUTAT                             AK      US    59.51   \n",
       "1028  PANT  70398  ANNETTE_ISLAND                      AK      US    55.05   \n",
       "1029  PASY  70414  SHEMYA_AFB                          AK      US    52.71   \n",
       "\n",
       "     longitude elevation  \n",
       "1011    156.79        19  \n",
       "1012    156.62         8  \n",
       "1013    162.63         5  \n",
       "1014    144.66       252  \n",
       "1015    165.43         7  \n",
       "1016    161.84        33  \n",
       "1017    155.61       103  \n",
       "1018    150.41       151  \n",
       "1019    147.88       134  \n",
       "1020     145.7       398  \n",
       "1021    145.96       573  \n",
       "1022    150.01        40  \n",
       "1023    170.22         6  \n",
       "1024    162.71        31  \n",
       "1025    156.67         8  \n",
       "1026     152.5        34  \n",
       "1027    139.67        12  \n",
       "1028    131.59        35  \n",
       "1029     174.1        31  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df['st/pr']=='AK') & (df['country']=='US'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('station_list.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
